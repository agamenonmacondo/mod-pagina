{
  "timestamp": "2025-05-16T14:27:33.355259",
  "topics": [
    {
      "title": "Large language model comparisons between English and Chinese query performance for cardiovascular prevention - Nature",
      "keywords": [
        "language",
        "large",
        "english",
        "model",
        "chinese"
      ],
      "relevance_score": 0.9,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "Large language model comparisons between English and Chinese query performance for cardiovascular prevention | Communications Medicine",
        "content": "Communications Medicine volume 5, Article number: 177 (2025) Cite this article Large language model (LLM) offer promise in addressing layperson queries related to cardiovascular disease (CVD) prevention. However, the accuracy and consistency of information provided by current general LLMs remain unclear. We evaluated capabilities of BARD (Google’s bidirectional language model for semantic understanding), ChatGPT-3.5, ChatGPT-4.0 (OpenAI’s conversational models for generating human-like text) and ERNIE (Baidu’s knowledge-enhanced language model for context understanding) in addressing CVD prevention queries in English and Chinese. 75 CVD prevention questions were posed to each LLM. The primary outcome was the accuracy of responses (rated as appropriate, borderline, inappropriate). For English prompts, the chatbots’ appropriate ratings are as follows: BARD at 88.0%, ChatGPT-3.5 at 92.0%, and ChatGPT-4.0 at 97.3%. All models demonstrate temporal improvement in initially suboptimal responses, with BARD and ChatGPT-3.5 each improving by 67% (6/9 and 4/6), and ChatGPT-4.0 achieving a 100% (2/2) improvement rate. Both BARD and ChatGPT-4.0 outperform ChatGPT-3.5 in recognizing the correctness of their responses. For Chinese prompts, the “appropriate” ratings are: ERNIE at 84.0%, ChatGPT-3.5 at 88.0%, and ChatGPT-4.0 at 85.3%. However, ERNIE outperform ChatGPT-3.5 and ChatGPT-4.0 in temporal improvement and self-awareness of correctness. For CVD prevention queries in English, ChatGPT-4.0 outperforms other LLMs in generating appropriate responses, temporal improvement, and self-awareness. The LLMs’ performance drops slightly for Chinese queries, reflecting potential language bias in these LLMs. Given growing availability and accessibility of LLM chatbots, regular and rigorous evaluations are essential to thoroughly assess the quality and limitations of the medical information they provide across widely spoken languages. Recently there has been an increase in the use of large language model (LLM) chatbots by patients seeking medical information. However, the accuracy of information provided by LLMs across different languages remain unclear. This study aimed to evaluate the performance of popular LLM chatbots, such as BARD, ChatGPT-3.5, ChatGPT-4.0, and ERNIE, in answering cardiovascular disease prevention questions in both English and Chinese. We tested these models with 75 questions each, focusing on the accuracy of their responses and their ability to improve over time. The results showed that ChatGPT-4 provided the most accurate answers in English and demonstrated the best improvement over time. In Chinese, ERNIE performed better in improving its responses over time. This research highlights the need for ongoing evaluations to ensure the spread of reliable health information by LLMs across diverse languages. Large language models (LLMs) consist of a neural network with typically billions of parameters, trained on large quantities of available text including those relevant to health care1,2,3,4. With the rapid development of LLMs, along with mass scale of data available for training, it is now possible for LLM to provide relatively appropriate answers and responding in a human-like manner when prompted with health-related queries5,6,7. LLM Chatbots such as BARD (Google’s bidirectional language model for semantic understanding) (https://bard.google.com/), ERNIE (Baidu’s knowledge-enhanced language model for context understanding) (https://wenxin.baidu.com/ernie3), and ChatGPT (OpenAI’s conversational language models for generating human-like text) (https://chat.openai.com/chat) are now readily accessible by public users8. In a time where the internet is becoming a go-to source for healthcare information9, these widely available and human-like LLM chatbots are set to serve as a resource for a broad range of individuals and communities. However, it remains uncertain how consistently these chatbots provide accurate and evidence-based information. Cardiovascular disease (CVD) prevention is a topic with extensive evidence-based information. Given the expansive burden of CVD globally10, LLM chatbots could assist in reducing associated inequities through broader access to high-quality health information on CVD prevention11,12. Early exploratory analyses showed that ChatGPT-3.5 is able to address CVD prevention queries with an appropriateness level of up to 88%6,8. Beyond ChatGPT-3.5, the relative performances of other common LLM Chatbots in this context have yet been evaluated13,14,15,16,17,18,19. As LLMs undergo refinement through user interactions and feedback, updated data, and underlying algorithm updates, they could improve over time. Additionally, LLM has the ability to mitigate hallucinations and false claims through self-checking20. Therefore, in addition to assessing how different LLM chatbots compare to each other in terms of appropriately answer queries, it is also important to gauge their potential ability to identify and rectify initial inappropriate responses through self-checking and to evaluate the temporal improvement of these models. Overall, understanding the extent to which some LLM chatbots are able to deliver reliable medical information while minimizing the spread of medical misinformation requires robust evaluation. In this study, we conduct a rigorous evaluation of widely accessible LLM chatbots, assessing their accuracy, temporal improvement, and self-checking capabilities in responding to CVD prevention queries in two predominant languages - English and Chinese. The results indicate that ChatGPT-4 delivers the most accurate answers in English and exhibits the greatest improvement over time, while ERNIE shows better performance in enhancing its responses in Chinese. To ensure a comprehensive evaluation that includes common and popular LLM-chatbots, we included four LLM Chatbots in our study: (1) ChatGPT-3.5 by OpenAI; (2) ChatGPT-4.0 by OpenAI; (3) BARD by Google; and (4) ERNIE by Baidu. The evaluation of English prompts, involved ChatGPT 3.5, ChatGPT 4, and BARD; for Chinese prompts, the evaluation involved ChatGPT 3.5, ChatGPT 4, and ERNIE (Supplementary Table S1). We used the models with their default configurations and temperature settings (temperature of 0.7 for ChatGPT-3.5 and -4.0; 0.5 for BARD; and 0.8 for ERNIE).We did not make any adjustments to these parameters during our analysis. This study does not include human or animal subjects, and so the need for ethical review was waived. The American College of Cardiology and American Heart Association provide guidelines and recommendations for CVD preventionss6,21,22. These guidelines encompass information on risk factors, diagnostic tests, and treatment options, as well as patient education and self-management strategies. Drawing from key topics within these guidelines, we involved 2 experienced attending-level cardiologists (YWC, HWJ) to generate questions related to CVD prevention, framing them similarly to how patients would inquire with physicians to ensure relevance and comprehensibility from a patient’s perspective (Supplementary Table S2). This patient-centered and guideline-based approach yielded a final set of 300 questions covering domains such as biomarker, medication information, dyslipidemia, hypertension, diet counseling, diabetes mellitus and/or chronic kidney disease, secondary prevention, prevention strategy, inflammation, exercise counseling, obesity, and tobacco treatment. We then translated these questions into Chinese, ensuring the appropriate use of conventional units (e.g., mg/dL) and international units (e.g., mmol/L) in the English and Chinese versions respectively Figure 1 depicts the comprehensive study design employed in our research. To enhance the reliability and minimize bias, a random sampling approach was implemented. Our target sample size was determined to be 225 (75 per chatbot), aiming for a 90% power to detect an effect size of 5% within a general linear model framework. Consequently, we randomly selected 75 questions from the original pool of 300 questions (Supplementary Table S3). Between the dates of 24th April and 9th May, separate query sessions were conducted for ChatGPT-3.5 (both English and Chinese versions), ChatGPT-4.0 (both English and Chinese versions), BARD (English only), and ERNIE (Chinese only). Each chatbot was utilized to respond to 75 prompts, with each prompt being posed once on the interface during the respective query session (Supplementary Fig. S1). This process yielded a total of 75 responses per chatbot. Hence, the final sample consisted of 75:75:75 responses generated by the three LLM-Chatbots, for both the English and Chinese sections of the study. The responses were independently evaluated by two panels of cardiologists from Singapore and China, each panel comprising cardiologists with a minimum of five years of practice in cardiology. One panel assessed the English responses, and the other evaluated the Chinese responses, ensuring proficiency in the respective languages. Sample strategy, preprocess, randomly-ordered assessment by blinded cardiologist. 75 questions were randomly selected from the original pool of 300 questions. Each chatbot was utilized to respond to 75 prompts, with each prompt being posed once on the interface during the respective query session. The evaluation was conducted in a blinded and randomly ordered manner. Specifically, the responses from three chatbots were randomly shuffled within the question set. The responses from three chatbots were randomly assigned to 3 rounds, in a 1:1:1 ratio, for blinded assessment by three cardiologists, with a 48-hour wash-out interval in between rounds so as to mitigate recency bias. To ensure the graders were unable to distinguish the origin of the response among different LLM Chatbots, we manually concealed any chatbot-specific features. These features included phrases such as “I am not a doctor” by GPT-4, which could indicate the use of a specific model. (Supplementary Table S4). As shown in Fig. 1, the evaluation was conducted in a blinded and randomly ordered manner. Specifically, the responses from three chatbots were randomly shuffled within the question set. The responses from three chatbots were randomly assigned to 3 rounds, in a 1:1:1 ratio, for blinded assessment by three cardiologists, with a 48-hour wash-out interval in between rounds so as to mitigate recency bias. The primary outcome in this study was the performance in responding to primary CVD prevention questions. Specifically, we used a two-step approach... [Contenido truncado]",
        "images": [
          "https://media.springernature.com/full/nature-cms/uploads/product/commsmed/header-95117eaa448f002964e5ef781247b926.svg"
        ],
        "url": "https://www.nature.com/articles/s43856-025-00802-0"
      }
    },
    {
      "title": "AI’s Spontaneously Develop Social Norms Like Humans - Neuroscience News",
      "keywords": [
        "research",
        "news",
        "neuroscience",
        "develop",
        "spontaneously"
      ],
      "relevance_score": 0.8,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "AI's Spontaneously Develop Social Norms Like Humans - Neuroscience News",
        "content": "Summary: Large language model (LLM) AI agents, when interacting in groups, can form shared social conventions without centralized coordination. Researchers adapted a classic “naming game” framework to test whether populations of AI agents could develop consensus through repeated, limited interactions.\n\nThe results showed that norms emerged organically, and even biases formed between agents, independent of individual behavior. Surprisingly, small subgroups of committed agents could tip the entire population toward a new norm, mirroring human tipping-point dynamics.\n\nA new study suggests that populations of artificial intelligence (AI) agents, similar to ChatGPT, can spontaneously develop shared social conventions through interaction alone.\n\nThe research from City St George’s, University of London and the IT University of Copenhagen suggests that when these large language model (LLM) artificial intelligence (AI) agents communicate in groups, they do not just follow scripts or repeat patterns, but self-organise, reaching consensus on linguistic norms much like human communities.\n\nThe study has been published today in the journal, Science Advances.\n\nLLMs are powerful deep learning algorithms that can understand and generate human language, with the most famous to date being ChatGPT.\n\n“Most research so far has treated LLMs in isolation,” said lead author Ariel Flint Ashery, a doctoral researcher at City St George’s, “but real-world AI systems will increasingly involve many interacting agents.\n\n“We wanted to know: can these models coordinate their behaviour by forming conventions, the building blocks of a society? The answer is yes, and what they do together can’t be reduced to what they do alone.”\n\nIn the study, the researchers adapted a classic framework for studying social conventions in humans, based on the “naming game” model of convention formation.\n\nIn their experiments, groups of LLM agents ranged in size from 24 to 200 individuals, and in each experiment, two LLM agents were randomly paired and asked to select a ‘name’ (e.g., an alphabet letter, or a random string of characters) from a shared pool of options. If both agents selected the same name, they earned a reward; if not, they received a penalty and were shown each other’s choices.\n\nAgents only had access to a limited memory of their own recent interactions—not of the full population—and were not told they were part of a group.\n\nOver many such interactions, a shared naming convention could spontaneously emerge across the population, without any central coordination or predefined solution, mimicking the bottom-up way norms form in human cultures.\n\nEven more strikingly, the team observed collective biases that couldn’t be traced back to individual agents.\n\n“Bias doesn’t always come from within,” explained Andrea Baronchelli, Professor of Complexity Science at City St George’s and senior author of the study, “we were surprised to see that it can emerge between agents—just from their interactions. This is a blind spot in most current AI safety work, which focuses on single models.”\n\nIn a final experiment, the study illustrated how these emergent norms can be fragile: small, committed groups of AI agents can tip the entire group toward a new naming convention, echoing well-known tipping point effects – or ‘critical mass’ dynamics – in human societies.\n\nThe study results were also robust to using four different types of LLM called Llama-2-70b-Chat, Llama-3-70B-Instruct, Llama-3.1-70BInstruct, and Claude-3.5-Sonnet respectively.\n\nAs LLMs begin to populate online environments – from social media to autonomous vehicles – the researchers envision their work as a steppingstone to further explore how human and AI reasoning both converge and diverge, with the goal of helping to combat some of the most pressing ethical dangers posed by LLM AIs propagating biases fed into them by society, which may harm marginalised groups.\n\nProfessor Baronchelli added: “This study opens a new horizon for AI safety research. It shows the dept of the implications of this new species of agents that have begun to interact with us—and will co-shape our future.\n\n“Understanding how they operate is key to leading our coexistence with AI, rather than being subject to it. We are entering a world where AI does not just talk—it negotiates, aligns, and sometimes disagrees over shared behaviours, just like us.”\n\nAuthor: Dr Shamim QuadirSource: City St. George’s, University of LondonContact: Dr Shamim Quadir – City St. George’s, University of LondonImage: The image is credited to Neuroscience News\n\nOriginal Research: Open access.“Emergent Social Conventions and Collective Bias in LLM Populations” by Andrea Baronchelli et al. Science Advances\n\nEmergent Social Conventions and Collective Bias in LLM Populations\n\nSocial conventions are the backbone of social coordination, shaping how individuals form a group.\n\nAs growing populations of artificial intelligence (AI) agents communicate through natural language, a fundamental question is whether they can bootstrap the foundations of a society.\n\nHere, we present experimental results that demonstrate the spontaneous emergence of universally adopted social conventions in decentralized populations of large language model (LLM) agents.\n\nWe then show how strong collective biases can emerge during this process, even when agents exhibit no bias individually.\n\nLast, we examine how committed minority groups of adversarial LLM agents can drive social change by imposing alternative social conventions on the larger population.\n\nOur results show that AI systems can autonomously develop social conventions without explicit programming and have implications for designing AI systems that align, and remain aligned, with human values and societal goals.",
        "images": [
          "https://neurosciencenews.com/files/2020/07/Neuroscience-News_AZ_RevNewest-scaled-e1596228355273-1.jpg"
        ],
        "url": "https://neurosciencenews.com/ai-llm-social-norms-28928/"
      }
    },
    {
      "title": "Meta Reportedly Delays 'Behemoth' AI Model: What This Could Mean for Its AI Tools - CNET",
      "keywords": [
        "behemoth",
        "delays",
        "model",
        "reportedly",
        "llama"
      ],
      "relevance_score": 0.7,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "Meta Reportedly Delays 'Behemoth' AI Model: What This Could Mean for Its AI Tools - CNET",
        "content": "Meta reportedly has pushed back the release of its Behemoth large language model for its artificial intelligence tools, delaying it until the fall. Behemoth was originally planned to release in April to coincide with Meta's first AI conference, LlamaCon, but it was delayed until June before this latest delay, according to a report by The Wall Street Journal on Thursday.\n\nMeta released Llama 4 in April. Llama -- Large Language Model Meta AI -- is Meta's family of LLMs. But Meta AI engineers are concerned the capabilities in the Behemoth LLM aren't a significant enough improvement over what's already available via Llama 4 to warrant a June release, according to the Journal, which cites unnamed sources within Meta.\n\nA large language model is what sits behind the chatbot interface you interact with when you ask AI tools like ChatGPT, Gemini and Meta AI a question. LLMs understand and generate human-like text.\n\nBehemoth is \"one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models,\" Meta said in April.\n\nThe tech giant aims to become one of the world's biggest AI providers and has already woven AI into how people interact on many of its apps across Facebook, Instagram, WhatsApp and Messenger, including helping with writing posts and captions and editing your images. Meta also released a standalone app for Meta AI at the end of April, which includes a hub for its Ray-Ban smart glasses.\n\nCNET social media and AI reporter Katelyn Chedraoui, who reported on LlamaCon in April, said this fresh delay adds to concerns that Meta is already behind the curve as far as what its AI tools are capable of.\n\n\"Meta's reported decision to delay the release of its new Llama models means the company could fall even further behind other big companies like OpenAI and Google,\" Chedraoui said. \"The race to build advanced and affordable AI is extremely competitive. The concern for Meta is how far ahead competitors will advance while it's still working on refining the models it has already announced.\"\n\nA representative for Meta didn't immediately respond to a request for comment.",
        "images": [
          "https://www.cnet.com/a/img/resize/1f5ac4bfd53fae579b690984fa86ee969ca0abad/hub/2020/09/11/5885bcea-a2b8-4e37-ac5f-7c17f2efcdf4/corinne-reichert-headshot.jpg?auto=webp&fit=crop&height=96&width=96"
        ],
        "url": "https://www.cnet.com/tech/services-and-software/meta-reportedly-delays-behemoth-ai-model-what-this-could-mean-for-its-ai-tools/"
      }
    }
  ]
}