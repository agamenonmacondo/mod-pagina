{
  "timestamp": "2025-05-31T21:17:22.901748",
  "topics": [
    {
      "title": "My Word: The virtues and vices of the virtual world - The Jerusalem Post",
      "keywords": [
        "vices",
        "intelligence",
        "write",
        "prevalent",
        "word"
      ],
      "relevance_score": 0.9,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "My Word: The virtues and vices of the virtual world - The Jerusalem Post",
        "content": "‘To err is human,” goes the popular saying. But that doesn’t mean artificial intelligence (AI) can’t make mistakes. It’s neither perfect, nor divine.I recently took a short introductory course to AI at a local community center. The program was aimed at senior citizens in a fast-changing world. Technophobe that I am, I went mainly to try to conquer my fears.Although I can see the value of AI – and realize that you can no more ignore it than get by in the modern world without a smartphone – I also see its inherent dangers. Perhaps my greatest takeaway from the course was a quote by Jon Kabat-Zinn: “You can’t stop the waves, but you can learn to surf.” So I surf the web, and use ChatGPT, and try to keep my head above the water. Artificial intelligence programs are so prevalent that when I called up Word to write this column, an AI prompt immediately suggested: “Describe what you want to write.”Yet I didn’t give in to temptation and ask the computer to do the work for me in the time it would take me to make a cup of coffee. I wrote, rewrote, trimmed, and edited this the old-fashioned way. Artificial intelligence (Illustrative). (credit: PIXABAY)As part of the course, we learned how to use AI programs to write a poem, put it to music, and create an avatar to perform it. The process didn’t take long, but what it provided in instant gratification, it lacked in emotional satisfaction. The result, however good, did not come from the heart. It was more artificial than authentic.IF YOU’RE into futuristic, dystopian movies, watch I, Robot. The 2004 film was ahead of its time. I was reminded of engineer Blake Lemoine’s warning in June 2022 that Google AI program LaMDA (Language Model for Dialogue Applications) was close to being sentient – with a built-in fear of dying. Google fired the whistleblower rather than pull the plug on the program. This week, I read a disconcerting Huffington Post piece which concluded that an Amazon-backed AI model “would try to blackmail engineers who threatened to take it offline.” “In tests, Anthropic’s Claude Opus 4 would resort to ‘extremely harmful actions’ to preserve its own existence, a safety report revealed.”It is disturbing to see AI programs writing not only themselves but also offering their own versions of history and general knowledge.Relying on previously presented material – some drawn from the dark world of fake news and conspiracy theories – over time, AI programs can change what is recorded in the future. When a lie is repeated often enough, conventional wisdom can turn into unconventional warfare.We are in a brave new world where seeing isn’t believingIt is easy to share photos, including fake images, and shut down discussion of what is really going on. Such falsehoods can inspire violence and terror attacks. When Elias Rodriguez murdered young diplomats Yaron Lischinsky and Sarah Milgrim in Washington last week, his shouts of “Free, free Palestine” continued to echo on social media after the sound of the gunshots had faded.There is a constant battle between facts and fallacies. The toxic effect of the “Muhammad al-Dura incident” (when the death of a 12-year-old caught in crossfire with Palestinians was falsely blamed on Israel) is still felt nearly 25 years later. The lies about Israel hitting Gaza’s al-Ahli Hospital early in this war have never been fully laid to rest, despite evidence that it was caused by a failed Palestinian rocket launch on Israel.The claim by a top UN official appearing on the BBC last week that 14,000 babies in Gaza faced imminent death within 48 hours has not disappeared. The UN itself clarified, when pressed, that it referred to a threat of malnutrition, not death, and was predicted over a period of more than a year without aid. Yet the story continued to circulate even after the two days passed without the threatened thousands of fatalities.This week, clearly AI-generated images appeared in the press and on social media purporting to show the bodies of nine children of Gazan doctor Alaa al-Najjar, who were reportedly killed by an Israeli drone while the pediatrician was at work. Other photos showing the same children had previously been used to illustrate different stories of purported atrocities.As the HonestReporting watchdog noted: “It’s also important to ask why all the sources of this sad story are secondary at best (the relatives) or agenda-driven at worst (Hamas Health Ministry officials)... any journalist should have asked why al-Najjar’s house was targeted, given that the IDF has made clear that it targets terrorists, not the civilians they hide behind.“When such questions are not asked, the result is irresponsible reporting that takes Hamas’s word as gospel and does further injustice to those it uses as human shields. It also exploits the faith of news consumers who believe they get all the facts from a reliable source.”The reason for the war – the Hamas invasion and mega-atrocity on October 7, 2023, in which 1,200 people in Israel were murdered and 251 abducted – has been replaced by new images, many of them false. This reduces the pressure on Hamas to end the war and release the remaining hostages – those being tortured and starved in terror tunnels, and the bodies of those killed and being held as bargaining chips.I ASKED ChatGPT 4 about the dangers of AI, and it swiftly compiled a list divided into key categories and subcategories. These included “job displacement” and “widening inequality” as the economic benefits could “be concentrated in the hands of a few companies or individuals”; biased data and opaque decision-making; “privacy and surveillance” – governments and corporations can use AI to monitor individuals at an unprecedented scale; data misuse; and misinformation and manipulation.AI can create “deepfakes,” highly realistic fake videos, audio, or images that can be used for fraud, political manipulation, or blackmail. AI-generated content and automated propaganda produced by bots can flood social media, spreading misinformation, disinformation, and influencing public opinion.AI can be used in weapons systems that make life-and-death decisions without human oversight, potentially leading to unintended mass casualties. And AI can intensify cyberattacks.The program carried on, succinctly summing up its own faults: “Loss of human control: A powerful AI pursuing goals that are not aligned with human values could act in harmful or unpredictable ways. Existential risk: Some experts warn that superintelligent AI – if it surpasses human intelligence – could pose a risk to humanity if not properly aligned and controlled.”Over-reliance on AI may erode critical skills, including human expertise, judgment, and capabilities, the system informed me. And blind trust in AI systems can lead to complacency and “critical errors in fields like healthcare, aviation, or defense.”“Mitigating these risks involves responsible development, transparent regulation, public awareness, and ensuring human-centered AI design. The benefits of AI are vast, but only if its development is handled with care,” it concluded.IN ISRAEL HAYOM last weekend, psychologist Ran Puni interviewed Eran Katz, an author and memory artist. Among other things, he holds the Guinness Book of Records for “Best Memory Stunt,” having recited 500 numbers forward and backward after hearing them only once. At 60, he gives workshops and demonstrates memory-improving techniques for seniors.“Memory defines us, makes us what we are, and without it, we are nothing,” Katz observed. “Humanity is becoming less intelligent because we have become addicted to technology and AI engines.“In the past, using memory was much more natural; we would simply use memory because we had no other choice. The Romans, our ancient sages, and even the first generations in the Land of Israel did not have smartphones or laptops. They had to rely solely on the brain to remember. Today, we are in a different situation.”Hadera Magistrates’ Court Judge Ehud Kaplan this month threw out a case after concurring with the defendant’s lawyer, who “suspected the police response was generated by ChatGPT. The cited legal clauses don’t exist.” The police admitted “there was a mistake.”AI is programmed to be nice; it needs to please to keep humans engaged. After presenting me the list of hazards, the program politely added: “Let me know if you’d like more details on any specific danger.”I decided to call it a day – before the artificial crystal ball could show me something too futuristic. I recently took a short introductory course to AI at a local community center. The program was aimed at senior citizens in a fast-changing world. Technophobe that I am, I went mainly to try to conquer my fears.Although I can see the value of AI – and realize that you can no more ignore it than get by in the modern world without a smartphone – I also see its inherent dangers. Perhaps my greatest takeaway from the course was a quote by Jon Kabat-Zinn: “You can’t stop the waves, but you can learn to surf.” So I surf the web, and use ChatGPT, and try to keep my head above the water. Artificial intelligence programs are so prevalent that when... [Contenido truncado]",
        "images": [
          "https://www.facebook.com/tr?id=1730128020581377&ev=PageView&noscript=1"
        ],
        "url": "https://www.jpost.com/opinion/article-856010"
      }
    },
    {
      "title": "New Artificial Intelligence Framework Centers on Civil Rights - GovTech",
      "keywords": [
        "intelligence",
        "leadership",
        "center",
        "conference",
        "artificial"
      ],
      "relevance_score": 0.8,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "New Artificial Intelligence Framework Centers on Civil Rights",
        "content": "Menu Show Search CONTINUE TO SITE ✕ IE 11 Not Supported For optimal browsing, we recommend Chrome, Firefox or Safari browsers.\n\n Artificial Intelligence New Artificial Intelligence Framework Centers on Civil Rights The Leadership Conference’s Center for Civil Rights and Technology’s new Innovation Framework aims to guide the responsible public- and private-sector development, investment and use of artificial intelligence systems.\n\n May 30, 2025 • News Staff Facebook LinkedIn Twitter Print Email Shutterstock The Center for Civil Rights and Technology (Center), a joint technology and civil rights advocacy hub created by The Leadership Conference on Civil and Human Rights and The Leadership Conference Education Fund, has unveiled a framework to guide artificial intelligence advancement.\n\nThe Leadership Conference on Civil and Human Rights is a coalition to protect U.\n\nS.\n\n residents’ civil rights, with members representing more than 240 national organizations.\n\n The Leadership Conference Education Fund is its education and research arm, founded in 1969 to help build public will for public policy centered on civil rights.\n\nIn an uncertain era of AI regulation, frameworks can provide a road map to mitigate risk during AI implementation.\n\n The National Institute of Standards and Technology released an AI risk management framework, for example, which is informing decision-making in states including Colorado.\n\n The new Innovation Framework aims to offer entities guidance for how they invest in, create and use AI systems to ensure civil rights are protected.\n\n“American-made AI will succeed when our rights lead the way,” Maya Wiley, Leadership Conference president and CEO, said in a statement.\n\n Securing AI leadership status for the U.\n\nS.\n\n is one of the current presidential administration’s priorities.\n\nThe framework lays out four foundational values to support a business strategy: centering civil and human rights in the design process, considering AI as a tool rather than a solution, human impact and oversight being integral to AI, and ensuring AI is environmentally sustainable.\n\n Ten life cycle pillars outlined in the framework aim to ensure the values are included in practice.\n\n For example, AI design should start with identifying appropriate use cases.\n\n Historically marginalized populations should be centered in this process.\n\n Responsible AI development should involve representative data, and it should include protections for sensitive data.\n\n AI tools should be assessed for bias and potential discriminatory impacts, and even after deployment, there should be consistent monitoring and mechanisms for accountability.\n\n“Private industry doesn’t have to wait on Congress or the White House to catch up; they can start implementing this Innovation Framework immediately,” Koustubh “K.\n\nJ.\n\n” Bagchi, Center vice president, said in a statement.\n\nThe framework aims to be a resource for companies, civil society and others advocating for responsible private-sector AI development and deployment.\n\n It can be used by AI investors, developers and deployers, including C-suite leaders, product teams and engineers.\n\nThe framework was created by gathering input from stakeholders and holding feedback sessions involving the civil rights community, the Center’s Advisory Council, and companies.\n\n Facebook LinkedIn Twitter Print Email Tags: Artificial Intelligence Emerging TechState GovernmentLocal GovernmentPolicy News Staff See More Stories by News Staff Never miss a story with the GovTech Today newsletter.\n\n SUBSCRIBE",
        "images": [
          "https://erepublic.brightspotcdn.com/bc/a8/3ad2250148b8a28b31d4bd4edd24/gt-with-block.svg"
        ],
        "url": "https://www.govtech.com/artificial-intelligence/new-artificial-intelligence-framework-centers-on-civil-rights"
      }
    },
    {
      "title": "What Happens When AI Replaces Workers? - Time Magazine",
      "keywords": [
        "work",
        "replaces",
        "people",
        "build",
        "train"
      ],
      "relevance_score": 0.7,
      "category": "General",
      "full_content": {
        "success": true,
        "title": "What Happens When AI Replaces Workers? | TIME",
        "content": "On Wednesday, Anthropic CEO Dario Amodei declared AI could eliminate half of all entry level white collar jobs within five years. Last week, a senior LinkedIn executive reported that AI is already starting to take jobs from new grads. In April, Fiverr’s CEO made it clear: “AI is coming for your job. Heck, it’s coming for my job too.” Even the new Pope is warning about AI’s dramatic potential to reshape our economy.\n\nThe stated goal of the major AI companies is to build artificial general intelligence, or AGI, defined as “a highly autonomous system that outperforms humans at most economically valuable work.”\n\nThis isn’t empty rhetoric—companies are spending over a trillion dollars to build towards AGI. And governments around the world are supporting the race to develop this technology.\n\nThey’re on track to succeed. Today’s AI models can score as well as humans on many standardized tests. They are better competitive programmers than most programming professionals. They beat everyone except the top experts in science questions.\n\nAs a result, AI industry leaders believe they could achieve AGI sometime between 2026 and 2035.\n\nAmong insiders at the top AI companies, it’s the near-consensus opinion that the day of most people’s technological unemployment, where they lose their jobs to AI, will arrive soon. AGI is coming for every part of the labor market. It will hit white collar workplaces first, and soon after will reach blue collar workplaces as robotics advances.\n\nIn the post-AGI world, an AI can likely do your work better and cheaper than you. While training a frontier AI model is expensive, running additional copies of it is cheap, and the associated costs are rapidly getting cheaper.\n\nA commonly proposed solution for an impending era of technological unemployment is government-granted universal basic income (UBI). But this could dramatically change how citizens participate in society because work is most people’s primary bargaining chip. Our modern world is upheld with a simple exchange: you work for someone with money to pay you, because you have time or skills that they don’t have.\n\nThe economy depends on workers’ skills, judgment, and consumption. As such, workers have historically bargained for higher wages and 40-hour work weeks because the economy depends on them.\n\nWith AGI, we are posed to change, if not entirely sever, that relationship. For the first time in human history, capital might fully substitute for labor. If this happens, workers won’t be necessary for the creation of value because machines will do it better and cheaper. As a result, your company won’t need you to increase their profits and your government won’t need you for their tax revenue.\n\nWe could face what we call “The Intelligence Curse”, which is when powerful actors such as governments and companies create AGI, and subsequently lose their incentives to invest in people.\n\nJust like in oil-rich states afflicted with the “resource curse,” governments won’t have to invest in their populations to sustain their power. In the worst case scenario, they won’t have to care about humans, so they won’t.\n\nBut our technological path is not predetermined. We can build our way out of this problem.\n\nMany of the people grappling with the other major risks from AGI—that it goes rogue, or helps terrorists create bioweapons, for example—focus on centralizing and regulatory solutions: track all the AI chips, require permits to train AI models. They want to make sure bad actors can’t get their hands on powerful AI, and no one accidentally builds AI that could literally end the world.\n\nHowever, AGI will not just be the means of mass destruction—it will be the means of production too. And centralizing the means of production is not just a security issue, it is a fundamental decision about who has power.\n\nWe should instead avert the security threats from AI by building technology that defends us. AI itself could help us make sure the code that runs our infrastructure is secure from attacks. Investments in biosecurity could block engineered pandemics. An Operation Warp Speed for AI alignment could ensure that AGI doesn’t go rogue.\n\nAnd if we protect the world against the extreme threats that AGI might bring about, we can diffuse this technology broadly, to keep power in your hands.\n\nWe should accelerate human-boosting AI over human-automating AI. Steve Jobs once called computers “bicycles for the mind,” after the way they make us faster and more efficient. With AI, we should aim for a motorcycle for the mind, rather than a wholesale replacement of it.\n\nThe market for technologies that keep and expand our power will be tremendous. Already today, the fastest-growing AI startups are those that augment rather than automate humans, such as the code editor Cursor. And as AI gets ever more powerful and autonomous, building human-boosting tools today could set the stage for human-owned tools tomorrow. AI tools could capture the tacit knowledge visible to you every day and turn it into your personal data moat.\n\nThe role of the labor of the masses can be replaced either with the AI and capital of a few, or the AI and capital of us all. We should build technologies that let regular people train their own AI models, run them on affordable hardware, and keep control of their data—instead of everything running through a few big companies. You could be the owner of a business, deploying AI you control on data you own to solve problems that feel unfathomable to you today.\n\nYour role in the economy could move from direct labor, to managing AI systems like the CEO of a company manages their direct reports, to steering the direction of AI systems working for you like a company board weighing in on long-term direction.\n\nThe economy could run on autopilot and superhumanly fast. Even when AI can work better than you, if you own and control your piece of it, you could be a player with real power—rather than just hoping for UBI that might never come.\n\nTo adapt the words of G. K. Chesterton, the problem with AI capitalism is if there aren’t enough capitalists. If everyone owns a piece of the AI future, all of us can win.\n\nAnd of course, AGI will make good institutions and governance more important than ever. We need to strengthen democracy against corruption and the pull of economic incentives before AGI arrives, to ensure regular people can win if we reach the point where governments and large corporations don’t need us.\n\nWhat’s happening right now is an AGI race, even if most of the world hasn’t woken up to it. The AI labs have an advantage in AI, but to automate everyone else they need to train their AIs in the skills and knowledge that run the economy, and then go and outcompete the people currently providing those goods and services.\n\nCan we use AI to lift ourselves up, before the AI labs train the AIs that replace us? Can we retain control over the economy, even as AI becomes superintelligent? Can we achieve a future where power still comes from the people?",
        "images": [],
        "url": "https://time.com/7289692/when-ai-replaces-workers/"
      }
    }
  ]
}